{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "467f1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-line output support\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' #Default is 'last'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ff48bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas are required\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba5b009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing Settings\n",
    "import matplotlib.pyplot as plt\n",
    "# Normal display of Chinese in windows environment (not enough in Linux, need to modify the font and reload)\n",
    "plt.rcParams['font.sans-serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.labelcolor'] = 'black'\n",
    "#The negative sign of the number used to normalize the coordinate axis\n",
    "plt.rcParams['axes.unicode_minus']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "b480f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "# if you use VPN\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:64893\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:64893\"\n",
    "# set your own OPENAI KEY\n",
    "os.environ['OPENAI_API_KEY'] = \"<OPENAI_KEY>\"\n",
    "# set your own organization of the OPENAI account\n",
    "openai.organization = \"<ORGANIZATION>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca3cbc",
   "metadata": {},
   "source": [
    "# Completion request function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c6789a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-4\", \n",
    "                                 temperature=0, max_tokens=3000):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6a37c",
   "metadata": {},
   "source": [
    "# Sample selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642ef51-0e6a-422b-87ae-c5728a67c14e",
   "metadata": {},
   "source": [
    "## Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1d311a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elements = [\"Ti\",\"Mn\",\"Fe\",\"W\",\"Mo\",\"Sn\"]\n",
    "def element_path(element):\n",
    "    path = ['Ce' + i  + '.csv' for i in Elements]\n",
    "    Dict = dict(zip(Elements, path))\n",
    "    return Dict[element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c5e0bf59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\LMY99\\\\Desktop\\\\GPT project\\\\Model1-4\\\\model-3 (reasoning)\\\\demo_binary\\\\demos\\\\files'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\LMY99\\\\Desktop\\\\GPT project\\\\Model1-4\\\\model-3 (reasoning)\\\\demo_binary\\\\demos\\\\files'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set file_paths\n",
    "os.getcwd()\n",
    "os.chdir(r'C:\\Users\\LMY99\\Desktop\\GPT project\\Model1-4\\model-3 (reasoning)\\demo_binary\\demos\\files')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc1c9b",
   "metadata": {},
   "source": [
    "## Select samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4cf44853-83e3-400a-824a-5ac58ba6992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = ['min', '25%', '50%', '75%', 'max']\n",
    "molar_ratio_matrix = np.array(np.empty((6, 5)))\n",
    "ghsv_matrix = np.array(np.empty((6, 5)))\n",
    "\n",
    "for i, j in enumerate(Elements):\n",
    "    Data = pd.read_csv(element_path(j), encoding = 'utf-8')\n",
    "    Data.columns = ['NO','Co1', 'Ratio', 'Dry-T(M)',  'Cal-T(M)',\n",
    "        'GHSV', 'Reac-Temp',  'O2-Con','H2O-Con','SO2-Con', 'Performance']\n",
    "    Des = Data.describe()\n",
    "    for k in range(len(thres)):\n",
    "        molar_ratio_matrix[i, k] = Des['Ratio'][thres[k]]\n",
    "        ghsv_matrix[i, k] = Des['GHSV'][thres[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "215bda8d-b5f8-4838-bbb1-516d9b5c51df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_cap(factor, i, j, molar_ratio_matrix = molar_ratio_matrix, ghsv_matrix = ghsv_matrix):\n",
    "    matrix = molar_ratio_matrix if factor == 'Ratio' else ghsv_matrix\n",
    "    return matrix[i][j], matrix[i][j + 1]\n",
    "\n",
    "def sampling(data, column):\n",
    "    # sampled_data = data.sample(n=min(len(data), size))\n",
    "    sampled_data = data.sample(n = 1)\n",
    "    return sampled_data\n",
    "\n",
    "def renew_index(available_index, selected_index):\n",
    "    return [idx for idx in available_index if idx not in selected_index]\n",
    "\n",
    "def Ratio_GHSV(data, i):\n",
    "    samples = pd.DataFrame()\n",
    "    available_index = list(range(len(data)))\n",
    "    \n",
    "    for factor in ['Ratio', 'GHSV']:\n",
    "        for j in range(4):  # Three levels\n",
    "            low_bar, cap = low_cap(factor, i, j)\n",
    "            bounds_check = data[factor].between(low_bar, cap, inclusive=\"both\")\n",
    "            selected_index = data.index[bounds_check]\n",
    "            temp_samples = data.loc[selected_index]\n",
    "            sampled_data = sampling(temp_samples, factor)\n",
    "\n",
    "            samples = pd.concat((samples, sampled_data), axis=0)\n",
    "            available_index = renew_index(available_index, list(sampled_data.index))\n",
    "    return samples, available_index, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cec280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in range(0,5):\n",
    "    print(f'Run: {j}')\n",
    "    for i, k in enumerate(Elements):\n",
    "        ele_path = element_path(k) + '.csv'\n",
    "        Data = pd.read_csv(ele_path, encoding = 'utf-8')\n",
    "        Data.columns = ['NO','Co1', 'Ratio', 'Dry-T(M)', 'Cal-T(M)',\n",
    "        'GHSV', 'Reac-Temp', 'O2-Con','H2O-Con','SO2-Con', 'Performance']\n",
    "        timeout = 600\n",
    "        per_flag = 4\n",
    "        diver_flag = 8\n",
    "        start_time = time.time()\n",
    "        while True:\n",
    "            samples, avaliable_index, data = Ratio_GHSV(Data, i)\n",
    "            diver = len(np.unique(samples['Reac-Temp']))\n",
    "            per = np.sum(samples['Performance'] == 'Positive')\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start_time\n",
    "            if (diver == diver_flag) and (per == per_flag):\n",
    "                break\n",
    "            if ((diver < diver_flag) or (per != per_flag)) and (elapsed_time < timeout):\n",
    "                continue\n",
    "            if (elapsed_time >= timeout) and (diver < diver_flag):\n",
    "                diver_flag -= 1\n",
    "                start_time = time.time()\n",
    "        print(elapsed_time)\n",
    "        samples = pd.concat((samples, Data.loc[avaliable_index,:]), axis = 0)\n",
    "        name =  f\"{k}\" + f'_run{j}' + '.csv'\n",
    "        samples.to_csv(name, encoding = 'utf-8', index = None) \n",
    "        print(per, 8 - per)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c4827-0a3c-45f4-9fa7-584ae3d08739",
   "metadata": {},
   "source": [
    "# Feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d1d606bf-9a4e-4b5b-ae11-ebea179496dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"##\"\n",
    "def form_PROMPT(data):\n",
    "\n",
    "    unit_map = {\n",
    "        'Ratio': '',\n",
    "        'Dry-T(M)': '℃',\n",
    "        'Cal-T(M)': '℃',\n",
    "        'GHSV': 'h-1',\n",
    "        'Reac-Temp': '℃',\n",
    "        'O2-Con': 'vol%',\n",
    "        'H2O-Con': 'vol%',\n",
    "        'SO2-Con': 'vol%',\n",
    "    }\n",
    "\n",
    "    feature_sets = [\n",
    "        ['Ratio', 'Performance'],\n",
    "        ['Ratio', 'Reac-Temp', 'Performance'],\n",
    "        ['Ratio', 'Reac-Temp', 'GHSV', 'Performance'],\n",
    "        ['Ratio', 'Reac-Temp', 'GHSV', 'H2O-Con', 'SO2-Con', 'Performance'],\n",
    "        ['Ratio', 'Reac-Temp', 'GHSV', 'Dry-T(M)', 'Cal-T(M)', 'H2O-Con', 'SO2-Con', 'Performance'],\n",
    "        ['Ratio', 'Reac-Temp', 'GHSV', 'Dry-T(M)', 'Cal-T(M)', 'H2O-Con', 'SO2-Con', 'O2-Con', 'Performance'],\n",
    "    ]\n",
    "\n",
    "    Prompts = {str(index): [] for index in range(len(feature_sets))}\n",
    "    for index, feature_set in enumerate(feature_sets):\n",
    "        for _, row in data.iterrows():\n",
    "            info_parts = []\n",
    "            for feature in feature_set:\n",
    "                if feature == 'performance':\n",
    "                    info_parts.append(f\"{feature}: {row[feature]}\")\n",
    "                if feature == 'Ratio':\n",
    "                    info_parts.append(f\"Molar Ratio: Ce to {row['Co1']} = {row[feature]}\")\n",
    "                else:\n",
    "                    info_parts.append(f\"{feature}: {row[feature]}{unit_map.get(feature, '')}\")                                        \n",
    "            one_sample = \"; \".join(info_parts) \n",
    "            Prompts[str(index)].append(one_sample + delimiter)\n",
    "\n",
    "    return Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa48d2",
   "metadata": {},
   "source": [
    "# Ordered and Structured prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d9568",
   "metadata": {},
   "source": [
    "## System message function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8c71bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sys_message(element_number,j):\n",
    "    Catalyst = f'Ce{Elements[element_number]}'\n",
    "    if j == 0:\n",
    "        system_message = f\"\"\"\n",
    "\"As an expert in the field of selective catalytic reduction of NOx by NH3, you will be given a set of tested datapoints to reason \n",
    "the most Collet_data trade-off and correlated relationship or trend between various factors and the performance of the catalysts. \n",
    "\n",
    "Four Notes:\n",
    "\n",
    "1.In reasoning, all factors in current datapoints must be identified and considered. \n",
    "\n",
    "2.The datapoints will be provided in a delimited format using {delimiter} characters. \n",
    "\n",
    "3.The performance of the catalyst will be evaluated based on a positive or negative result. A positive result indicates a NO conversion rate of 95% or higher, while a negative result implies a NO conversion rate below 95%.\n",
    "\n",
    "4.The SO2 content and H2O content must be observed carefully, if there are. \n",
    "\n",
    "To generate the reasoning paths, you need to follow these guidelines:\n",
    "\n",
    "1. The reasoning paths should be presented in a logical and structured manner, in the form of step 1-N.\n",
    "\n",
    "2. The reasoning paths are designed to infer the performance of untested datapoints. Therefore, each step must include Collet_dataised and quantified content that facilitates inference. \n",
    "This step-by-step approach will allow for accurate predictions in untested experimental scenarios, considering global factors.\n",
    "\n",
    "3. The output content should only contain reasoning paths, in at most 2000 words, without any additional messages.\"\n",
    " \n",
    "    \"\"\"\n",
    "        return system_message\n",
    "    elif j > 0:\n",
    "         system_message = f\"\"\"\n",
    "\"You will be given a new set of tested datapoints to refine the existing reasoning paths by reasonsing\n",
    "the most Collet_data trade-off and correlated relationship or trend between various factors and the performance of the catalysts. .\n",
    "\n",
    "Four Notes:\n",
    "\n",
    "1.In reasoning, all factors in current datapoints must be identified and considered. \n",
    "\n",
    "2.The datapoints will be provided in a delimited format using {delimiter} characters. \n",
    "\n",
    "3.The performance of the catalyst will be evaluated based on a positive or negative result. \n",
    "A positive result indicates a NO conversion rate of 95% or higher, while a negative result implies a NO conversion rate below 95%.\n",
    "\n",
    "4.The SO2 content and H2O content must be observed carefully, if there are.\n",
    "\n",
    "To generate the reasoning paths, you need to follow these guidelines:\n",
    "\n",
    "1. The reasoning paths should be presented in a logical and structured manner, in the form of step 1-N.\n",
    "\n",
    "2. The reasoning paths are designed to infer the performance of untested datapoints. Therefore, each step must include Collet_dataised and quantified content that facilitates inference. \n",
    "This step-by-step approach will allow for accurate predictions in untested experimental scenarios, considering global factors.\n",
    "\n",
    "3. The output content should only contain reasoning paths, in at most 3000 words, without any additional messages.\"\n",
    " \n",
    "    \"\"\" \n",
    "    return system_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3188c6",
   "metadata": {},
   "source": [
    "## User message function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4e2ca55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_message(j,datapoints):\n",
    "    if j == 0:\n",
    "        user_message = f\"\"\"\n",
    "        Please see the provided datapoints below:\n",
    "\n",
    "        Datapoints:```{datapoints}```\n",
    "        \"\"\"\n",
    "        return user_message\n",
    "    elif j > 0:\n",
    "        user_message = f\"\"\"\n",
    "        Please see the provided existing reasoning paths, and new datapoints demilited by triple \n",
    "        backticks below:\n",
    "        \n",
    "        Existing reasoning paths: ```{response}```\\n\n",
    "        \n",
    "        New datapoints: ```{datapoints}```     \n",
    "        \n",
    "        \"\"\"\n",
    "        return user_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce83ce0-a2b3-4dbe-9d3d-b03e55f5576e",
   "metadata": {},
   "source": [
    "## OS-CoT Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2d745f46-ec78-4a56-9f33-5d81f413e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Respon_GPT4_OS = pd.DataFrame(np.em0pty((6,5)),columns = [f'run{i}' for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8f64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(5):\n",
    "    Collet_data = pd.DataFrame([])\n",
    "    for i in range(6):\n",
    "        name =  f\"{Elements[i]}_run{run}.csv\"        \n",
    "        Data = pd.read_csv(name, encoding = 'utf-8')\n",
    "        Data.columns = ['NO','Co1', 'Ratio',  'Dry-T(M)', , 'Cal-T(M)',\n",
    "            'GHSV', 'Reac-Temp',  'O2-Con','H2O-Con','SO2-Con' 'Performance']\n",
    "        Collet_data = pd.concat((Collet_data, Data.loc[0:7,:]), axis = 0)\n",
    "        Collet_data = Collet_data.reset_index(drop = True)\n",
    "        Prompt_data = form_PROMPT(Collet_data)\n",
    "    for j in range(0, 6):    \n",
    "        datapoints = Prompt_data[str(j)]\n",
    "        messages =  [{'role':'system','content': sys_message(i, j)},\n",
    "                     {'role':'user', 'content': user_message(j, datapoints)}] # call system_message&user_message function\n",
    "        response = get_completion_from_messages(messages) # call Completion request function\n",
    "        print(response)\n",
    "        Respon_GPT4_OS.loc[j, f'run{run}'] = response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f986b7c",
   "metadata": {},
   "source": [
    "# One-pot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705adcd-71c0-4420-80de-8f9500ad8f2b",
   "metadata": {},
   "source": [
    "## System message function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bb12fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sys_one_pot(i):\n",
    "    Catalyst = f'Ce{Elements[i]}'\n",
    "    system_message = system_message = f\"\"\"\n",
    "\"As an expert in the field of selective catalytic reduction of NOx by NH3, you will be given a set of tested datapoints to reason \n",
    "the most Collet_data trade-off and correlated relationship or trend between various factors and the performance of the catalysts. \n",
    "\n",
    "Four Notes:\n",
    "\n",
    "1.In reasoning, all factors in current datapoints must be identified and considered. \n",
    "\n",
    "2.The datapoints will be provided in a delimited format using {delimiter} characters. \n",
    "\n",
    "3.The performance of the catalyst will be evaluated based on a positive or negative result. A positive result indicates a NO conversion rate of 95% or higher, while a negative result implies a NO conversion rate below 95%.\n",
    "\n",
    "4.The SO2 content and H2O content must be observed carefully, if there are. \n",
    "\n",
    "To generate the reasoning paths, you need to follow these guidelines:\n",
    "\n",
    "1. The reasoning paths should be presented in a logical and structured manner, in the form of step 1-N.\n",
    "\n",
    "2. The reasoning paths are designed to infer the performance of untested datapoints. Therefore, each step must include Collet_dataised and quantified content that facilitates inference. \n",
    "This step-by-step approach will allow for accurate predictions in untested experimental scenarios, considering global factors.\n",
    "\n",
    "3. The output content should only contain reasoning paths, in at most 2000 words, without any additional messages.\"\n",
    " \n",
    "    \"\"\"\n",
    "    return system_message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1301d819-ed76-4f1d-8ff9-368528942b53",
   "metadata": {},
   "source": [
    "## User message function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5cb243f5-6b3d-4f75-b8a7-f601eeb3964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_one_pot(j,datapoints):\n",
    "    user_message = f\"\"\"\n",
    "        Please see the provided datapoints below:\n",
    "\n",
    "        Datapoints:```{datapoints}```\n",
    "        \"\"\"\n",
    "    return user_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf82606-b27c-438e-bded-e8f362ded3fb",
   "metadata": {},
   "source": [
    "## OP-CoT Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ac655896-82ae-4299-b17b-c53c16408d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Respon_GPT4_OP = pd.DataFrame(np.empty((1,5)),columns = [f'run{i}' for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8795fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(5):\n",
    "    Collet_data = pd.DataFrame([])\n",
    "    for i in range(6):\n",
    "        name =  f\"{Elements[i]}_run{run}.csv\"        \n",
    "        Data = pd.read_csv(name, encoding = 'utf-8')\n",
    "        Data.columns = ['NO','Co1', 'Ratio', 'Dry-T(M)', 'Cal-T(M)',\n",
    "            'GHSV', 'Reac-Temp', 'O2-Con','H2O-Con','SO2-Con', 'Performance']\n",
    "        Collet_data = pd.concat((Collet_data, Data.loc[0:8,:]), axis = 0)\n",
    "        Collet_data = Collet_data.reset_index(drop = True)\n",
    "        Prompt_data = form_PROMPT(Collet_data)\n",
    "    datapoints = Prompt_data[str(5)]\n",
    "    print(datapoints)\n",
    "    messages =  [{'role':'system','content': sys_message(i, j)},\n",
    "                 {'role':'user', 'content': user_message(j, datapoints)}] # call system_message&user_message function\n",
    "    response = get_completion_from_messages(messages) # call Completion request function\n",
    "    print(response)\n",
    "    Respon_GPT4_OP.loc[0, f'run{run}'] = response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59d351",
   "metadata": {},
   "source": [
    "# Inferring performance of binary ce-based oxides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "531958ca-bb79-42a4-a0ce-724964f00d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"##\"\n",
    "def form_PROMPT_infer(data):\n",
    "    Answer = list(data['Performance'])\n",
    "    unit_map = {\n",
    "        'Ratio': '',\n",
    "        'Dry-T(M)': '℃',\n",
    "        'Cal-T(M)': '℃',\n",
    "        'GHSV': 'h-1',\n",
    "        'Reac-Temp': '℃',\n",
    "        'O2-Con': 'vol%',\n",
    "        'H2O-Con': 'vol%',\n",
    "        'SO2-Con': '',\n",
    "    }\n",
    "\n",
    "    feature_set = ['Ratio', 'Reac-Temp', 'GHSV', 'Dry-T(M)', 'Cal-T(M)', 'H2O-Con', 'SO2-Con', 'O2-Con']\n",
    "\n",
    "    Prompts = []\n",
    "    for _, row in data.iterrows():\n",
    "        info_parts = []\n",
    "        for index, feature in enumerate(feature_set):\n",
    "            if feature == 'Ratio':\n",
    "                info_parts.append(f\"Molar Ratio: Ce to {row['Co1']} = {row[feature]}\")\n",
    "            else:\n",
    "                info_parts.append(f\"{feature}: {row[feature]}{unit_map.get(feature, '')}\")                                        \n",
    "        one_sample = \"; \".join(info_parts) \n",
    "        Prompts.append(one_sample + delimiter)\n",
    "\n",
    "    return Prompts, Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "54f80834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_message(element_number, datapoints, run):\n",
    "    cata = f'Ce{Elements[element_number]}'\n",
    "    run = f'run{run}'\n",
    "    user_message = f\"\"\"\n",
    "    Below are the reasoning paths for evaluating the performance of catalysts:\n",
    "        \n",
    "    {Respon_GPT4_OS.loc[5, run]}\n",
    "\n",
    "    Additionally, here are the five untested samples for the {cata} catalyst:\n",
    "\n",
    "    {datapoints}\n",
    "\n",
    "    \"\"\"\n",
    "    return user_message   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8afc6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sys_message(element_number):\n",
    "    cata = f'Ce{element_number}'\n",
    "    system_message = f\"\"\"\n",
    "As an AI expert in the field of selective catalytic reduction of NOx by NH3, you will be given five untested samples of {cata} catalyst along with reasoning paths for evaluating catalyst performance. \n",
    "The datapoints will be separated by {delimiter} characters, and the performance metrics are either positive or negative.\n",
    "\n",
    "Your task is to infer the most likely performance of the provided untested datapoints of {cata} catalyst. Your inference process should thoroughly consider the provided reasoning paths.\n",
    "\n",
    "Please present the output in the following format for each datapoint with very concise reasoning paths, in at most 15 words:\n",
    "\n",
    "{delimiter}<performance>{delimiter}<reasoning paths>\n",
    "{delimiter}<performance>{delimiter}<reasoning paths>\n",
    "......\n",
    "{delimiter}<performance>{delimiter}<reasoning paths>\n",
    "\n",
    "Remember not to make a decision on the performance until you have carefully considered all reasoning paths.\"\"\"\n",
    "    return system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "eae5dc82-4e57-4436-ac9f-b736e2ddb53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(response):\n",
    "    res_split = response.split(delimiter)\n",
    "    collect_per = []\n",
    "    for item in res_split:\n",
    "        if (item == 'Positive') or (item == 'Negative'):\n",
    "            collect_per.append(item)\n",
    "    if len(collect_per) == Batch:\n",
    "        tem_check = np.sum(collect_per == answer[idx: idx + Batch])\n",
    "        return tem_check\n",
    "    else:\n",
    "        print('the LLM has been malfunctioned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d4d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(5):\n",
    "    for i in range(6):\n",
    "        name =  f\"{Elements[i]}_run{run}.csv\"        \n",
    "        Data = pd.read_csv(name, encoding = 'utf-8')\n",
    "        Data.columns = ['NO','Co1', 'Ratio', \n",
    "                        'Dry-T(M)', 'Cal-T(M)',\n",
    "                         'GHSV', 'Reac-Temp','O2-Con','H2O-Con','SO2-Con', \n",
    "                        'Performance']\n",
    "        Infer_datapoints, Answer = form_PROMPT_infer(Data.loc[8:58,:])\n",
    "        Batch = 5\n",
    "        num = 0\n",
    "        for idx in range(0, 50 - Batch, Batch):\n",
    "            datapoints = Infer_datapoints[idx: idx + Batch]\n",
    "            messages =  [{'role':'system', 'content': sys_message(i)},\n",
    "                         {'role':'user', 'content': user_message(i, datapoints, run)}]\n",
    "            response = get_completion_from_messages(messages)\n",
    "            tem_check = parser(response)\n",
    "            if tem_check:\n",
    "                num += tem_check\n",
    "        print('Inferrance Accuracy is {}%'.format(num/50) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e72039",
   "metadata": {},
   "source": [
    "# Inferring performance of ternary oxides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a6e2ee02-23e8-4e07-9e75-fcc5f9c05ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\LMY99\\\\Desktop\\\\GPT project\\\\Model1-4\\\\model-3 (reasoning)\\\\demo_binary\\\\demos\\\\files\\\\ternary_test'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r'C:\\Users\\LMY99\\Desktop\\GPT project\\Model1-4\\model-3 (reasoning)\\demo_binary\\demos\\files\\ternary_test')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e183380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Combinations = [\"Ti_Fe\",\"Ti_Mo\",\"Ti_Sn\",\"Ti_W\",\"Mn_Fe\",\"Mn_Sn\",\"Mn_Ti\",\"Mn_W\"]\n",
    "def combination_path(combination):\n",
    "    \n",
    "    path = [f'Ce' + i.split('_')[0] + i.split('_')[1]  + '.csv' for i in Combinations]\n",
    "    Dict = dict(zip(Combinations, path))\n",
    "    return Dict[combination]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "263a5bdc-af44-4404-91e0-931f504d60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"##\"\n",
    "def form_PROMPT_infer(data):\n",
    "    Answer = list(data['Performance'])\n",
    "    unit_map = {\n",
    "        'Ratio1': '',\n",
    "        'Ratio2': '',\n",
    "        'Dry-T(M)': '℃',\n",
    "        'Cal-T(M)': '℃',\n",
    "        'GHSV': '',\n",
    "        'Reac-Temp': '℃',\n",
    "        'O2-Con': 'vol%',\n",
    "        'H2O-Con': 'vol%',\n",
    "        'SO2-Con': 'vol%',\n",
    "    }\n",
    "\n",
    "    feature_set = ['Ratio1', 'Ratio2', 'Reac-Temp', 'GHSV', 'Dry-T(M)', 'Cal-T(M)', 'H2O-Con', 'SO2-Con', 'O2-Con']\n",
    "\n",
    "    Prompts = []\n",
    "    for _, row in data.iterrows():\n",
    "        info_parts = []\n",
    "        for index, feature in enumerate(feature_set):\n",
    "            if feature == 'Ratio1':\n",
    "                info_parts.append(f\"Molar Ratio1: Ce to {row['Co1']} = {row[feature]}\")\n",
    "            elif feature == 'Ratio2':\n",
    "                info_parts.append(f\"Molar Ratio2: Ce to {row['Co2']} = {row[feature]}\")\n",
    "            else:\n",
    "                info_parts.append(f\"{feature}: {row[feature]}{unit_map.get(feature, '')}\")                                        \n",
    "        one_sample = \"; \".join(info_parts) \n",
    "        Prompts.append(one_sample + delimiter)\n",
    "\n",
    "    return Prompts, Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "46fed929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sys_message(cata1,cata2):\n",
    "    Combination = f'Ce{cata1}{cata2}'\n",
    "    system_message = f\"\"\"\n",
    "As an AI specialist in selective catalytic reduction of NOx by NH3, you will receive untested datapoints of ternary Ce-based mixed metal oxides catalyst. \n",
    "And \n",
    "You will be provided with reasoning paths for evaluating the performance of binary Ce-based mixed metal oxides catalyst. \n",
    "Performance metrics are Positive and Negative.\n",
    "\n",
    "Your objective is to infer the most likely performance of the untested datapoints for Ce{cata1}{cata2} catalyst. \n",
    "This involves meticulous weighing of the provided reasoning paths of evaluating performance of binary Ce-based mixed metal oxides catalysts:\n",
    "\n",
    "The output must only contain inferred performance (Positive or Negative) on each datapoint in the following formats with your concise inference process, in at most 25 words.\n",
    "    \n",
    "    {delimiter}<performance>{delimiter}<reasoning paths>\n",
    "    {delimiter}<performance>{delimiter}<reasoning paths>\n",
    "    ......\n",
    "    {delimiter}<performance>{delimiter}<reasoning paths>\n",
    "    \"\"\"\n",
    "    return system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "53e4753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_message(cata1,cata2, datapoints, run):\n",
    "    run = f'run{run}'\n",
    "    user_message = f\"\"\"\n",
    "    Here are the reasoning paths of binary Ce-based mixed metal oxides catalysts: \n",
    "\n",
    "    reasoning paths: \\n\n",
    "    <{Respon_GPT4_OS.loc[5, run]}>\\n\n",
    "\n",
    "    Here are the untested datapoints for the ternary mixed oxides catalyst, Ce{cata1}{cata2}, delimited with {delimiter} characters:\n",
    "\n",
    "    Untested datapoints: \n",
    "    ```{datapoints}```\n",
    "    \n",
    "    Please ensure thorough analysis before determining performance. \n",
    "    \"\"\"\n",
    "    return user_message       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18041238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(5):\n",
    "    for i in range(8):        \n",
    "        Data = pd.read_csv(combination_path(Combinations[i]), encoding = 'utf-8')\n",
    "        Data.columns = ['NO','Co1','Co2', 'Ratio1', 'Ratio2', \n",
    "                        'Dry-T(M)', 'Dry-t(M)', 'Cal-T(M)',\n",
    "                         'GHSV', 'Reac-Temp',\n",
    "                        'O2-Con','H2O-Con','SO2-Con', \n",
    "                        'Performance']\n",
    "        Infer_datapoints, Answer = form_PROMPT_infer(Data)\n",
    "        catas = Combinations[i].split('_')\n",
    "        cata1, cata2 = catas[0], catas[1]\n",
    "        Batch = 5\n",
    "        num = 0\n",
    "        for idx in range(0, 50 - Batch, Batch):\n",
    "            datapoints = Infer_datapoints[idx: idx + Batch]\n",
    "            messages =  [{'role':'system', 'content': sys_message(cata1, cata2)},\n",
    "                         {'role':'user', 'content': user_message(cata1, cata2, datapoints, run)}]\n",
    "            response = get_completion_from_messages(messages)\n",
    "            tem_check = parser(response)\n",
    "            if tem_check:\n",
    "                num += tem_check\n",
    "        print('Inferrance Accuracy is {}%'.format(num/50) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69877fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e5a3e-d76b-48a8-bea9-94839bf4b305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a5819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
